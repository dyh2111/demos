{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dyh2111/demos/blob/main/Practical_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🧠 Retrieval-Augmented Generation (RAG): A Simplified Practical Walkthrough\n",
        "\n",
        "ver 1.0 public\n",
        "\n",
        "**by Dan Harvey**  \n",
        "📧 dan [at] danielyusay.com  \n",
        "📧 daniel.harvey [at] columbia.edu  \n",
        "\n",
        "I create these notebooks primarily for myself — as compact, hands-on implementations of foundational ideas in areas I find interesting, relevant, or worth exploring. They're meant as learning exercises and quick prototypes, not in-depth or production-ready guides.\n",
        "\n",
        "If you found this helpful or have constructive suggestions for improvement, feel free to reach out. I’d love to hear from you.\n"
      ],
      "metadata": {
        "id": "CHsUiaxsv7hj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Intro\n",
        "\n",
        "This notebook presents a functional and simplified implementation of Retrieval-Augmented Generation (RAG), combined with chain-of-thought (CoT) style prompt to enhance clarity and structure in generated responses.  \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 🔄 Demo Outline\n",
        "\n",
        "1. **User Prompt**  \n",
        "   - ❓ Accepts natural language questions through an input field.\n",
        "   - Ensures flexibility for any query the user wants to ground in documents.\n",
        "\n",
        "2. **Upload Reference Document**  \n",
        "   - 📂 Supports `.txt` and `.pdf` file uploads.\n",
        "   - Parses and preprocesses the content for embedding and retrieval.\n",
        "\n",
        "3. **Prompt + Context → LLM**  \n",
        "   - 🧩 Retrieves relevant snippets from the uploaded document using simple semantic matching or heuristics.\n",
        "   - Combines prompt and retrieved context into a well-structured input for the LLM.\n",
        "\n",
        "4. **Answer / Inference Output**  \n",
        "   - 🧠 Uses a local LLM (`Qwen3-4B`) to generate grounded, context-aware responses.\n",
        "   - Presents structured output in a clean display.\n",
        "\n",
        "---\n",
        "\n",
        "In production-grade RAG systems — as outlined in the foundational [RAG paper (Lewis et al., 2020)](https://arxiv.org/abs/2005.11401) — documents are typically uploaded in batches, tokenized, embedded (i.e., converted into vector representations), and stored in a vector database such as Pinecone or Google’s Vertex AI Vector Search. These vectors are then queried at runtime using top-k similarity search (e.g., cosine, inner product, or Euclidean distance) to retrieve relevant context, which is combined with the user’s prompt to supplement downstream generation.\n",
        "\n",
        "In the original RAG framework, the retrieval component used token or segment based retrieval, while the generation component was based on pretrained models like BERT and BART.\n",
        "\n",
        "In this notebook, we simulate a simplified RAG-style pipeline specifically designed to answer questions on individual, user-uploaded documents.\n",
        "\n",
        "---\n",
        "\n",
        "To ensure accessibility, all models and components in this tutorial are optimized to run on Google Colab with a T4 GPU.\n",
        "\n",
        "This walkthrough is ideal for anyone looking to understand or prototype the fundamentals of retrieval-augmented workflows.\n"
      ],
      "metadata": {
        "id": "QE1zaVRlX_gz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📌 Example Use Case\n",
        "\n",
        "A practical example in a professional setting might involve an employee interacting with an internal company chatbot to ask questions about benefits — such as health insurance, PTO policies, or parental leave. Instead of relying on generic answers, the system retrieves relevant sections from internal documents like the Employee Handbook, HR policy PDFs, or onboarding materials.\n",
        "\n",
        "**In this demo, we have linked a sample PDF resume and will ask a question about the person's education.**"
      ],
      "metadata": {
        "id": "Y4ts5mppfaFj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "\n",
        "- [Pinecone Documentation – Guides](https://docs.pinecone.io/guides/)  \n",
        "  Official guide to Pinecone, a vector database for large-scale similarity search.\n",
        "\n",
        "- [Johnson, Jeff, et al. \"Billion-scale similarity search with GPUs.\"](https://arxiv.org/abs/2005.11401)  \n",
        "  *arXiv preprint arXiv:2005.11401* (2020). Describes FAISS, an efficient similarity search library using GPUs.\n",
        "\n",
        "- [FAISS: A Library for Efficient Similarity Search – Facebook Engineering](https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/)  \n",
        "  Facebook’s engineering blog post introducing FAISS for scalable vector similarity search.\n",
        "\n",
        "- [Mistral Cookbook: basic_RAG.ipynb](https://github.com/mistralai/cookbook/blob/main/mistral/rag/basic_RAG.ipynb)  \n",
        "  A minimal working example of Retrieval-Augmented Generation (RAG) from the Mistral team."
      ],
      "metadata": {
        "id": "cA4xKxsFaZU6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation"
      ],
      "metadata": {
        "id": "O8-5MCtja62y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run Once\n",
        "!pip install hf_xet # Hugging Face Fast Transfer util\n",
        "!pip install pymupdf # PDF parser"
      ],
      "metadata": {
        "id": "bfgLA9G2lHbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Log into HF, or use HF_TOKEN\n",
        "#from huggingface_hub import notebook_login\n",
        "#notebook_login()"
      ],
      "metadata": {
        "id": "e5vJyuyxlT-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dependencies/Libraries\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TextIteratorStreamer\n",
        "import urllib.request\n",
        "from IPython.display import display, Markdown, clear_output\n",
        "import ipywidgets as widgets\n",
        "from google.colab import files\n",
        "import pymupdf\n",
        "import ipywidgets as widgets\n",
        "import os\n",
        "import subprocess\n",
        "from threading import Thread\n",
        "import sys\n",
        "import time"
      ],
      "metadata": {
        "id": "Lf0XGoIok8O9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🤖 Load the LLM\n",
        "\n",
        "**Note:** This notebook is designed with Google Colab in mind, and the selected model runs comfortably on a T4 GPU.\n",
        "\n",
        "I'm using **Qwen3-4B** — a compact yet highly capable model that is based on the Llama architecture.\n",
        "\n",
        "**I've recently been enjoying the work coming out of the Qwen lab.**\n",
        "\n",
        "The team has done an impressive job distilling performance into smaller footprints, and their models consistently perform well on benchmarks, including tasks like HellaSwag.\n",
        "\n",
        "For lightweight, local inference setups, Qwen models strike a strong balance between efficiency and output quality.\n",
        "\n",
        "- 🔗 [Qwen Models on Hugging Face](https://huggingface.co/Qwen)  \n",
        "- 💬 [Official Qwen Site](https://qwen.ai/)\n",
        "\n",
        "\n",
        "> **Note:** This model's theoretical maximum input length is **40,960 tokens**, meaning it can process sequences of that length in a single forward pass.  However, in practice, the usable input length depends heavily on factors such as available GPU memory, batch size, tokenizer settings, and backend optimizations. Most real-world implementations handle long sequences using more efficient memory strategies or windowed attention.\n"
      ],
      "metadata": {
        "id": "a_43H6kYkmhe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feel free to experiment with others\n",
        "model_name = \"Qwen/Qwen3-4B\"\n",
        "\n",
        "# Make sure you have an instance with a GPU\n",
        "if torch.cuda.is_available():\n",
        "    print(\"✅ GPU available\")\n",
        "\n",
        "    # Load model and tokenizer from HF\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name,\n",
        "                                                torch_dtype=torch.float16,\n",
        "                                                device_map=\"cuda\",\n",
        "                                                trust_remote_code=True)\n",
        "\n",
        "    print(\"✅ LLM loaded.\")\n",
        "else:\n",
        "    print(\"❌ No GPU available\")\n",
        "    print(\"❌ LLM not loaded.\")\n"
      ],
      "metadata": {
        "id": "w_oqV0nxkpTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check GPU Usage after loading model - Should use 8520MiB or 8.3GB of VRAM\n",
        "str_output = subprocess.getoutput('nvidia-smi')\n",
        "print(str_output)"
      ],
      "metadata": {
        "id": "9Txyv0Fg0DJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Label for prompt\n",
        "prompt_label = widgets.Label(\"Enter your document-based question:\")\n",
        "\n",
        "# Text box (input field)\n",
        "question_box = widgets.Text(\n",
        "    value='What is the persons highest level of education and school?',\n",
        "    placeholder='What is the persons highest level of education and school?',\n",
        "    layout=widgets.Layout(width='100%')\n",
        ")\n",
        "\n",
        "# Submit button\n",
        "submit_button = widgets.Button(description=\"Submit\")\n",
        "\n",
        "# Output area\n",
        "output = widgets.Output()\n",
        "\n",
        "# Button click event handler\n",
        "def on_submit_clicked(b):\n",
        "    global user_question\n",
        "\n",
        "    with output:\n",
        "        output.clear_output()\n",
        "        user_question = question_box.value\n",
        "        print(f\"You asked: {user_question}\")\n",
        "\n",
        "submit_button.on_click(on_submit_clicked)\n",
        "\n",
        "# Display widgets in order\n",
        "display(prompt_label, question_box, submit_button, output)"
      ],
      "metadata": {
        "id": "cHuNZcpPmijZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the file from GitHub - or upload your own\n",
        "url = \"https://github.com/dyh2111/demos/raw/84047e890fe273607b1dd5532a8eca6bf7f47f07/RAG/sample_resume.pdf\"\n",
        "local_path = \"sample_resume.pdf\"\n",
        "urllib.request.urlretrieve(url, local_path)\n",
        "\n",
        "uploaded_file = local_path\n",
        "print(f\"📄 Uploaded file: {uploaded_file}\")\n",
        "\n",
        "file_extension = os.path.splitext(uploaded_file)[-1].lower()[1:]\n",
        "processed_text = \"\"\n",
        "\n",
        "# Parse file content based on extension\n",
        "if file_extension == \"txt\":\n",
        "    with open(uploaded_file, \"r\", encoding=\"utf-8\") as f:\n",
        "        processed_text = f.read()\n",
        "\n",
        "elif file_extension == \"pdf\":\n",
        "    doc = pymupdf.open(uploaded_file)  # PyMuPDF uses `fitz`\n",
        "    for page in doc:\n",
        "        processed_text += page.get_text()  # get_text() already returns a string\n",
        "\n",
        "else:\n",
        "    raise ValueError(f\"❌ Unsupported file format: {file_extension}\")\n",
        "\n",
        "# Output\n",
        "print(\"✅ Processed text loaded.\")"
      ],
      "metadata": {
        "id": "PWzmOD0Jl-AX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the prompt template - where we instruct the LLM how to respond to the provided info.\n",
        "# This was inspired by the Mistral RAG examples\n",
        "# Feel free to play around with this and see how it changes the response.\n",
        "\n",
        "prompt_template = f\"\"\"\n",
        "<|im_start|>system\n",
        "You are a researcher. Your task is to answer the provided **question (Q)** and **document context (C)**.\n",
        "\n",
        "When presented with a question, respond factually using only the information from the document. Cite where the information was found.\n",
        "If the answer is not found in the document, explicitly state that it could not be located.\n",
        "\n",
        "Make sure your answer is grounded in the document and follows a logical, informative tone.\n",
        "If there is a dangerous or ill-meaning question being asked, decline to answer and refer to Engineering.\n",
        "\n",
        "---\n",
        "/think\n",
        "\n",
        "# Instructions:\n",
        "\n",
        "1. **Summarize:**\n",
        "   Briefly explain the nature of the question and the general content of the document.\n",
        "\n",
        "2. **Reasoning:**\n",
        "   Describe the reasoning process used to arrive at the answer, including how the document supports the conclusion.\n",
        "\n",
        "3. **Provide the Answer:**\n",
        "   Clearly state the answer in a complete sentence and cite where the information was found.\n",
        "\n",
        "4. Conclude with a brief thank-you message.\n",
        "<|im_end|>\n",
        "\n",
        "<|im_start|>user\n",
        "# Question:\n",
        "{{Q}}\n",
        "\n",
        "# Context:\n",
        "{{C}}\n",
        "<|im_end|>\n",
        "---\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "DnofioeBEaQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RAG-style Inference\n",
        "\n",
        "# Insert the user question and retrieved context into the prompt template\n",
        "final_prompt = prompt_template.replace(\"{Q}\",\n",
        "                                       user_question).replace(\"{C}\",\n",
        "                                       processed_text[:10000])\n",
        "\n",
        "# Tokenize the prompt — converts text into token IDs (integers) for model input\n",
        "# The model will convert these token IDs into embeddings internally\n",
        "inputs = tokenizer(final_prompt,\n",
        "                   return_tensors=\"pt\",\n",
        "                   truncation=True,\n",
        "                   max_length=10500,\n",
        "                   padding=True,\n",
        "                   ).to(model.device)"
      ],
      "metadata": {
        "id": "hyvHEJkImIlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set to true if you want to see the actual input to the LLM.\n",
        "# This is helpful to debug\n",
        "if False:\n",
        "    print(\"Input to LLM:\\n\")\n",
        "    print(final_prompt)"
      ],
      "metadata": {
        "id": "kETOCDZz-jiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the streamer\n",
        "streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
        "\n",
        "# Set up generation arguments\n",
        "generation_args = dict(\n",
        "    input_ids=inputs[\"input_ids\"],\n",
        "    attention_mask=inputs[\"attention_mask\"],\n",
        "    max_new_tokens=300,\n",
        "    eos_token_id=[151645, 151643],\n",
        "    temperature=0.6,\n",
        "    top_k=2,\n",
        "    top_p=0.95,\n",
        "    streamer=streamer\n",
        ")\n",
        "\n",
        "# Launch generation in background thread to enable stream\n",
        "thread = Thread(target=model.generate, kwargs=generation_args)\n",
        "thread.start()\n",
        "\n",
        "# Stream + live-update Markdown cell\n",
        "response = \"\"\n",
        "display_handle = display(Markdown(\"\"), display_id=True)\n",
        "\n",
        "# For each token, we will print, clear, and update\n",
        "for token in streamer:\n",
        "    response += token\n",
        "    display_handle.update(Markdown(response))\n",
        "    time.sleep(0.01)\n",
        "\n",
        "# Also show final markdown version for pretty rendering (optional)\n",
        "#display(Markdown(response))\n"
      ],
      "metadata": {
        "id": "xkMJC_9W5Cen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# End of Demo\n",
        "\n",
        "Good luck with your RAG experiments and LLM adventures!  \n",
        "\n",
        "\n",
        "-Dan"
      ],
      "metadata": {
        "id": "0dc1UvHzX9XX"
      }
    }
  ]
}